#!/usr/bin/env python3
"""
Memory Agent
============

Primary memory coordination interface for TorinAI AGI system.
Coordinates hot tier (MySQL), cold tier, and semantic search (embeddings).

Architecture:
- Hot Tier: MySQL storage (0-60 days, fast access) - torinai_thinking_hot
- Cold Tier: MySQL storage (60+ days, archival) - torinai_memory_cold
- Embeddings: Semantic similarity search (all-MiniLM-L6-v2)

Features:
- CRUD operations (create, read, update, delete)
- Semantic and keyword-based search
- Automatic tier migration (hot → cold after 60 days)
- Governance integration (capability tokens for deletes)
- Protected parameter modifications (fail-closed security)

Integration:
- Single entry point exported from core/memory/__init__.py
- Protected against autonomous self-modification
- Constitutional constraints enforcement

Author: TorinAI System
Version: 8.0
"""

import asyncio
import logging
import uuid
from typing import Dict, Any, List, Optional, Set, Tuple
from datetime import datetime, timedelta

# Memory storage implementations
from core.memory.storage.mysql_storage import MySQLStorage
from core.memory.utils.embedding_service import (
    EmbeddingService,
    get_embedding_service
)
from core.memory.utils.interfaces import (
    MemoryItem,
    MemoryType,
    MemoryQuery,
    MemorySearchResult,
    MemoryOperation
)
from core.memory.utils.memory_worthiness import MemoryWorthinessMetadata

logger = logging.getLogger(__name__)


class MemoryAgent:
    """
    Memory Agent - Primary Memory Coordination Interface

    Coordinates hot tier (MySQL 0-60 days), cold tier (MySQL 60+ days),
    and semantic search capabilities for TorinAI memory system.

    Architecture:
        - MySQL Hot: Fast hot tier storage for recent memories (torinai_thinking_hot)
        - MySQL Cold: Cold tier archival for historical memories (torinai_memory_cold)
        - Embeddings: Semantic similarity search across tiers

    Governance:
        - Protected delete operations require capability tokens
        - Parameter modifications are governance-protected
        - Autonomous self-modification is blocked
    """

    def __init__(self):
        """
        Initialize Memory Agent

        Sets up storage backends but does not connect (call initialize())
        """
        # MySQL storage (handles both hot and cold tiers)
        self.mysql_storage: Optional[MySQLStorage] = None  # Hot+Cold tier (MySQL both)

        # Embedding service (sentence transformers)
        self.embedding_service: Optional[EmbeddingService] = None  # all-MiniLM-L6-v2
        self.embedding_dim: int = 384  # Embedding dimension

        # Memory cache (optional in-memory cache)
        self.memory_cache: Dict[str, MemoryItem] = {}  # memory_id → MemoryItem
        self.cache_enabled: bool = True  # Enable caching

        # Agent state
        self.initialized: bool = False

        # Autonomous background loops (persistent cognition)
        self.maintenance_loop_active: bool = False
        self.abstraction_loop_active: bool = False
        self.reflection_loop_active: bool = False
        self.abstraction_pipeline = None  # Will be initialized with hierarchical abstraction
        self.bayesian_beliefs = None  # Will be initialized with belief system

        # Performance metrics
        self.metrics = {
            "memories_stored": 0,
            "memories_retrieved": 0,
            "cache_hits": 0,
            "tier_migrations": 0,
            "queries_executed": 0,
            "consolidations_run": 0,
            "abstractions_formed": 0,
            "beliefs_updated": 0,
            "maintenance_cycles": 0
        }

    async def initialize(self) -> bool:
        """
        Initialize memory agent and all storage backends

        Connects to MySQL hot tier, MySQL cold tier, and loads embedding model.

        Returns:
            True if all components initialized successfully, False otherwise
        """
        if self.initialized:
            return True

        try:
            logger.info("Initializing MemoryAgent...")

            # Initialize MySQL storage (hot and cold tiers)
            try:
                self.mysql_storage = MySQLStorage()
                await self.mysql_storage.initialize()
                logger.info("✓ MySQL hot tier initialized (0-60 days)")
                logger.info("✓ MySQL cold tier available (60+ days)")
            except Exception:
                logger.error("Failed to initialize MySQL storage")

            # Initialize embedding service (all-MiniLM-L6-v2)
            try:
                self.embedding_service = get_embedding_service()
                if self.embedding_service.initialize():
                    logger.info("✓ Embedding service initialized (384-dim)")
                else:
                    logger.error("Embedding service initialization failed")
            except Exception as e:
                logger.error(f"Failed to initialize embedding service: {e}")

            # Initialize MySQL query agent (hot and cold tier queries)
            try:
                from core.memory.query.mysql_query_agent import get_query_agent
                self.mysql_query_agent = await get_query_agent()
                logger.info("✓ MySQL query agent initialized")
            except Exception:
                logger.error("Failed to initialize MySQL query agent")

            # Verify MySQL storage is available
            if not self.mysql_storage:
                logger.error("MySQL storage not available - MemoryAgent cannot function")
                return False

            # Initialize hierarchical abstraction pipeline
            try:
                from core.reasoning.bayesian_uncertainty import BayesianUncertaintySystem
                from core.reasoning.hierarchical_abstraction import initialize_abstraction_pipeline

                self.bayesian_beliefs = BayesianUncertaintySystem()
                self.abstraction_pipeline = initialize_abstraction_pipeline(
                    memory_agent=self,
                    uncertainty_system=self.bayesian_beliefs
                )
                logger.info("✓ Hierarchical abstraction pipeline initialized")
            except Exception as e:
                logger.warning(f"Hierarchical abstraction not available: {e}")

            self.initialized = True
            logger.info("MemoryAgent initialized successfully")

            # Start autonomous background loops (persistent cognition)
            await self.start_memory_loops()
            logger.info("✓ Autonomous cognitive loops started")

            return True

        except Exception as e:
            logger.error(f"MemoryAgent initialization failed: {e}")
            return False

    # ================================================================================================
    # MEMORY STORAGE (Hot Tier)
    # ================================================================================================

    async def store_memory(
        self,
        content: str,
        memory_type: Optional[MemoryType] = None,
        importance_score: float = 0.5,
        confidence_score: float = 1.0,
        tags: Optional[List[str]] = None,
        source_context: Optional[Dict[str, Any]] = None,
        embedding_metadata: Optional[Dict[str, Any]] = None,
        related_memories: Optional[List[str]] = None,
        decay_rate: Optional[float] = None,
        access_count: int = 0,
        session_id: Optional[str] = None,
        user_id: Optional[str] = None,
        reasoning_trace: Optional[List[str]] = None,
        thinking_state: Optional[Dict[str, Any]] = None,
        decision_factors: Optional[Dict[str, Any]] = None,
        emotional_context: Optional[Dict[str, Any]] = None
    ) -> Tuple[bool, Optional[str]]:
        """
        Store memory to hot tier (MySQL) with intelligent filtering

        Memory Agent analyzes raw inputs and generates MemoryWorthinessMetadata.
        Calling systems should NOT pre-generate metadata - that's Memory Agent's job.

        Args:
            content: Memory content (text) - REQUIRED
            memory_type: Type of memory (EPISODIC, SEMANTIC, etc.) - Optional, will be inferred
            importance_score: Importance score (0.0-1.0)
            confidence_score: Confidence in memory accuracy (0.0-1.0)
            tags: Optional tags for categorization
            source_context: Raw metadata about memory source (system analyzes this)
            embedding_metadata: Optional metadata for embedding
            related_memories: Optional list of related memory IDs
            decay_rate: Optional custom decay rate
            access_count: Initial access count
            session_id: Optional session identifier
            user_id: Optional user identifier
            reasoning_trace: Optional chain of thought steps (list of reasoning steps)
            thinking_state: Optional thinking state metadata (DEPRECATED - metadata generated here)
            decision_factors: Optional decision factors that influenced this memory
            emotional_context: Optional emotional/sentiment context

        Returns:
            Tuple of (success: bool, memory_id: Optional[str])
        """
        print(f"\n[MEMORY_AGENT.STORE_MEMORY] Called with:", flush=True)
        print(f"  Content length: {len(content)} chars", flush=True)
        print(f"  Memory type: {memory_type}", flush=True)
        print(f"  Tags: {tags}", flush=True)
        print(f"  Importance: {importance_score}", flush=True)
        print(f"  Confidence: {confidence_score}", flush=True)
        print(f"  Initialized: {self.initialized}", flush=True)

        if not self.initialized:
            print(f"[MEMORY_AGENT.STORE_MEMORY] Initializing memory agent...", flush=True)
            await self.initialize()
            print(f"[MEMORY_AGENT.STORE_MEMORY] Initialization complete", flush=True)

        # ========== STEP 1: GENERATE OR EXTRACT METADATA ==========
        print(f"[MEMORY_AGENT.STORE_MEMORY] STEP 1: Generate/extract metadata", flush=True)
        worthiness_metadata = None

        # Check if upstream system pre-generated metadata (DEPRECATED path)
        if thinking_state and "worthiness_metadata" in thinking_state:
            try:
                from core.memory.utils.memory_worthiness import MemoryWorthinessMetadata

                # Deserialize metadata from dict
                metadata_dict = thinking_state["worthiness_metadata"]
                worthiness_metadata = MemoryWorthinessMetadata.from_dict(metadata_dict)

                logger.debug(f"Extracted worthiness metadata from thinking_state (source: {worthiness_metadata.source_system})")

            except Exception as e:
                logger.warning(f"Failed to extract worthiness metadata: {e}")
                worthiness_metadata = None

        # PREFERRED PATH: Generate metadata from raw inputs
        if worthiness_metadata is None:
            try:
                print(f"[MEMORY_AGENT.STORE_MEMORY] Generating worthiness metadata...", flush=True)
                worthiness_metadata = await self._generate_worthiness_metadata(
                    content=content,
                    confidence_score=confidence_score,
                    tags=tags,
                    source_context=source_context,
                    reasoning_trace=reasoning_trace
                )
                logger.debug(f"Generated worthiness metadata from raw inputs (source: {worthiness_metadata.source_system})")
                print(f"[MEMORY_AGENT.STORE_MEMORY] Metadata generated: source={worthiness_metadata.source_system}", flush=True)

            except Exception as e:
                logger.error(f"Failed to generate worthiness metadata: {e}")
                print(f"[MEMORY_AGENT.STORE_MEMORY] ✗ Metadata generation failed: {e}", flush=True)
                # Fail open - allow storage without metadata
                worthiness_metadata = None

        # ========== STEP 2: EVALUATE FILTERING DECISION ==========
        print(f"[MEMORY_AGENT.STORE_MEMORY] STEP 2: Evaluating filter decision...", flush=True)
        print(f"  Metadata available: {worthiness_metadata is not None}", flush=True)

        if worthiness_metadata is not None:
            try:
                from core.memory.utils.memory_filter import get_memory_filter

                memory_filter = get_memory_filter()
                print(f"[MEMORY_AGENT.STORE_MEMORY] Memory filter: {type(memory_filter)}", flush=True)

                # Evaluate with optional reasoning trace for calibration
                decision = memory_filter.evaluate(
                    metadata=worthiness_metadata,
                    reasoning_trace=reasoning_trace
                )
                print(f"[MEMORY_AGENT.STORE_MEMORY] Filter decision complete", flush=True)

                # ========== STEP 3: HANDLE REJECTION ==========
                print(f"[MEMORY_AGENT.STORE_MEMORY] STEP 3: Filter decision - should_store={decision.should_store}", flush=True)
                if not decision.should_store:
                    logger.info(
                        f"✗ Memory REJECTED by filter: "
                        f"rule={decision.rule_matched}, "
                        f"rationale={decision.rationale}, "
                        f"source={worthiness_metadata.source_system}"
                    )

                    print(f"[MEMORY_AGENT.STORE_MEMORY] ✗ REJECTED by filter:", flush=True)
                    print(f"  Rule: {decision.rule_matched}", flush=True)
                    print(f"  Rationale: {decision.rationale}", flush=True)
                    print(f"  Source: {worthiness_metadata.source_system}", flush=True)

                    # Track rejection metrics
                    if not hasattr(self, 'filter_metrics'):
                        self.filter_metrics = {'rejections': 0, 'accepts': 0}
                    self.filter_metrics['rejections'] += 1

                    # Return early - do not store
                    return False, None

                # ========== STEP 4: HANDLE ACCEPTANCE ==========
                print(f"[MEMORY_AGENT.STORE_MEMORY] STEP 4: ✓ ACCEPTED by filter", flush=True)
                logger.info(
                    f"✓ Memory ACCEPTED by filter: "
                    f"rule={decision.rule_matched}, "
                    f"decision_type={decision.decision_type}, "
                    f"rationale={decision.rationale}"
                )

                # Track acceptance metrics
                if not hasattr(self, 'filter_metrics'):
                    self.filter_metrics = {'rejections': 0, 'accepts': 0}
                self.filter_metrics['accepts'] += 1

                # Freeze metadata to enforce immutability
                worthiness_metadata.freeze()

                # Update thinking_state with frozen metadata
                if thinking_state is None:
                    thinking_state = {}
                thinking_state["worthiness_metadata"] = worthiness_metadata.to_dict()
                thinking_state["filter_decision"] = {
                    "rule_matched": decision.rule_matched,
                    "decision_type": decision.decision_type,
                    "rationale": decision.rationale,
                    "confidence": decision.confidence
                }

            except Exception as e:
                logger.error(f"Memory filtering error: {e}")
                # On filter error, default to storing (fail-open for now)
                logger.warning("Defaulting to STORE on filter error (fail-open)")
        else:
            # No metadata provided - allow storage but log warning
            logger.warning(
                f"No worthiness metadata provided - storing without filtering. "
                f"Source systems should generate upstream metadata."
            )

        # ========== STEP 5: CHECK FOR DUPLICATES ==========
        # Search for similar memories to prevent duplicate storage
        print(f"[DUPLICATE CHECK] Checking for similar memories (threshold: 0.75)...", flush=True)
        similar_memories = await self._find_similar_memories(
            content=content,
            similarity_threshold=0.75,  # Lower threshold to catch more duplicates
            tags=tags,
            memory_type=memory_type
        )
        print(f"[DUPLICATE CHECK] Found {len(similar_memories)} similar memories", flush=True)

        if similar_memories:
            # Found similar memory - merge instead of creating duplicate
            print(f"[MEMORY_AGENT.STORE_MEMORY] STEP 5: Found {len(similar_memories)} duplicates - MERGING", flush=True)
            logger.info(
                f"Found {len(similar_memories)} similar memories (similarity >= 0.85), "
                f"merging instead of creating duplicate"
            )

            # Merge with most similar memory
            merged_memory_id = await self._merge_memory_content(
                existing_memory=similar_memories[0],
                new_content=content,
                new_metadata={
                    'importance_score': importance_score,
                    'confidence_score': confidence_score,
                    'tags': tags,
                    'source_context': source_context,
                    'reasoning_trace': reasoning_trace,
                    'thinking_state': thinking_state
                }
            )

            if merged_memory_id:
                logger.info(f"Memory merged into existing memory: {merged_memory_id}")
                print(f"[MEMORY_AGENT.STORE_MEMORY] ✓ MERGED into {merged_memory_id}", flush=True)
                return True, merged_memory_id
            else:
                logger.warning("Merge failed, proceeding with storage of new memory")
                print(f"[MEMORY_AGENT.STORE_MEMORY] Merge failed, creating new memory", flush=True)
        else:
            print(f"[MEMORY_AGENT.STORE_MEMORY] STEP 5: No duplicates found - proceeding with new storage", flush=True)

        # ========== STEP 6: PROCEED WITH STORAGE ==========
        memory_id = f"mem_{uuid.uuid4().hex}"
        print(f"[MEMORY_AGENT.STORE_MEMORY] Creating new memory with ID: {memory_id}", flush=True)
        created_at = datetime.now()

        # Infer memory_type if not provided
        if memory_type is None:
            # Infer from worthiness_metadata and source_context
            memory_type = self._infer_memory_type(worthiness_metadata, source_context or {})
            logger.debug(f"Inferred memory_type as {memory_type.value}")

        # Generate embedding if service available
        embedding = self.embedding_service.generate_embedding(content) if self.embedding_service else None
        logger.debug(f"[EMBEDDING DEBUG] Embedding service available: {self.embedding_service is not None}")
        logger.debug(f"[EMBEDDING DEBUG] Generated embedding: {embedding is not None}, size: {len(embedding) if embedding else 0}")

        # Create MemoryItem
        memory_item = MemoryItem(
            memory_id=memory_id,
            memory_type=memory_type,
            content=content,
            importance_score=importance_score,
            confidence_score=confidence_score,
            created_at=created_at,
            last_accessed=created_at,
            access_count=access_count,
            decay_rate=decay_rate or 0.01,
            tags=tags or [],
            metadata=source_context or {},
            embeddings=embedding,  # Use 'embeddings' (plural) to match MySQL storage
            embedding_metadata=embedding_metadata or {},
            related_memories=related_memories or [],
            session_id=session_id or '',
            user_id=user_id or '',
            tier='hot',  # Store to hot tier
            archived_at=None,
            deleted_at=None,
            # Chain of thought and cognitive state tracking
            reasoning_trace=reasoning_trace,
            thinking_state=thinking_state,  # Includes frozen metadata + filter decision
            decision_factors=decision_factors,
            emotional_context=emotional_context
        )

        try:
            # Store to MySQL hot tier
            print(f"[MEMORY_AGENT.STORE_MEMORY] STEP 6: Storing to MySQL hot tier...", flush=True)
            print(f"  Memory ID: {memory_id}", flush=True)
            print(f"  MySQL storage available: {self.mysql_storage is not None}", flush=True)

            success = await self.mysql_storage.store_memory(memory_item)

            print(f"[MEMORY_AGENT.STORE_MEMORY] MySQL storage result: {success}", flush=True)

            if success:
                self.metrics["memories_stored"] += 1
                logger.info(f"Memory {memory_id} stored to hot tier (filtered)")
                print(f"[MEMORY_AGENT.STORE_MEMORY] ✓ SUCCESS - Memory {memory_id} stored", flush=True)
                return True, memory_id
            else:
                logger.error(f"Failed to store memory {memory_id}")
                print(f"[MEMORY_AGENT.STORE_MEMORY] ✗ FAILED - MySQL returned success=False", flush=True)
                return False, None

        except Exception as e:
            logger.error(f"Memory storage error: {e}")
            print(f"[MEMORY_AGENT.STORE_MEMORY] ✗ EXCEPTION: {e}", flush=True)
            import traceback
            traceback.print_exc()
            return False, None

    async def store_batch(self, memories: List[MemoryItem]) -> Tuple[bool, int]:
        """
        Store multiple memories in batch (optimized)

        Generates embeddings in batch for efficiency.

        Returns:
            Tuple of (success, count_stored)
        """
        if not self.initialized:
            await self.initialize()

        # Generate batch embeddings
        contents = [m.content for m in memories]
        embeddings = self.embedding_service.batch_embed(
            contents
        ) if self.embedding_service else [None] * len(memories)

        # Assign embeddings to memories
        for memory, embedding in zip(memories, embeddings):
            memory.embedding = embedding

        # Batch store to MySQL
        results = await self.mysql_storage.store_batch(
            memories=memories,
            batch_size=100
        )

        if results:
            self.metrics["memories_stored"] += len(memories)
            return True, len(memories)
        else:
            return False, 0

    async def _generate_worthiness_metadata(
        self,
        content: str,
        confidence_score: float,
        tags: Optional[List[str]],
        source_context: Optional[Dict[str, Any]],
        reasoning_trace: Optional[List[str]]
    ) -> 'MemoryWorthinessMetadata':
        """
        Generate MemoryWorthinessMetadata from raw inputs.

        This is where Memory Agent analyzes the raw data and makes admission decisions.

        Args:
            content: The memory content
            confidence_score: Confidence score
            tags: Tags from source system
            source_context: Raw metadata from source system
            reasoning_trace: Reasoning steps

        Returns:
            MemoryWorthinessMetadata
        """
        from datetime import datetime
        from core.memory.utils.memory_worthiness import (
            MemoryWorthinessMetadata,
            CognitionMetadata,
            NoveltyMetadata,
            CriticalityMetadata,
            QueryMetadata,
            OutcomeMetadata,
            TemporalMetadata,
            JustificationMetadata,
            DecisionType,
            ConsequenceLevel,
            PatternType,
            QueryType,
            ReusabilityLevel,
            DomainImportance
        )

        source_context = source_context or {}
        source_system = source_context.get("source_system", "unknown")

        # Analyze content for error patterns
        content_lower = content.lower()
        is_error = any(phrase in content_lower for phrase in [
            "sorry", "cannot see", "not visible", "unable to",
            "can't see", "image you intended", "please upload",
            "i'm sorry", "i can't", "i cannot"
        ])

        # Extract key metrics
        reasoning_step_count = len(reasoning_trace) if reasoning_trace else 0
        has_vision = source_context.get("has_image", False) or source_context.get("has_video", False)
        context_count = source_context.get("context_count", 0)

        # Check tags for semantic hints (used throughout metadata generation)
        tags_lower = [t.lower() for t in (tags or [])]
        has_research_tags = any(tag in tags_lower for tag in ['research', 'analysis', 'investigation', 'findings'])
        has_structural_tags = any(tag in tags_lower for tag in ['structure', 'architecture', 'dependencies', 'patterns'])

        # 1. Cognition Metadata
        complexity_score = 0.0
        if reasoning_step_count > 0:
            complexity_score += min(reasoning_step_count / 5.0, 0.4)
        if context_count > 0:
            complexity_score += min(context_count / 10.0, 0.3)
        if has_vision and not is_error and confidence_score > 0.5:
            complexity_score += 0.3  # Vision only if successful
        # BOOST: Research/structural findings are inherently complex
        if has_research_tags or has_structural_tags:
            print(f"[METADATA DEBUG] Boosting complexity_score for research/structure tags", flush=True)
            complexity_score += 0.7  # Ensure it passes the 0.6 soft threshold
        complexity_score = min(complexity_score, 1.0)

        print(f"[METADATA DEBUG] Complexity score: {complexity_score:.2f}", flush=True)
        print(f"[METADATA DEBUG] Reasoning steps: {reasoning_step_count}", flush=True)
        print(f"[METADATA DEBUG] Has research tags: {has_research_tags}", flush=True)
        print(f"[METADATA DEBUG] Has structural tags: {has_structural_tags}", flush=True)

        cognition = CognitionMetadata(
            reasoning_steps=reasoning_step_count,
            reasoning_depth=min(reasoning_step_count, 3),
            execution_time_ms=0.0,
            inference_count=reasoning_step_count,
            complexity_score=complexity_score,
            required_backtracking=False,
            used_multiple_strategies=False,
            uncertainty_resolved=confidence_score > 0.7 and not is_error
        )

        # 2. Novelty Metadata
        novelty = NoveltyMetadata(
            is_novel=False,  # Would require memory lookup
            contradicts_existing=False,
            synthesis_of_domains=[],
            pattern_type=PatternType.ROUTINE,
            first_occurrence=False,
            connects_disparate_knowledge=False
        )

        # 3. Criticality Metadata

        # Research findings have higher reusability
        reusability = ReusabilityLevel.HIGH if (has_research_tags or has_structural_tags) else ReusabilityLevel.MEDIUM

        criticality = CriticalityMetadata(
            decision_type=DecisionType.OPERATIONAL,
            domain_importance=DomainImportance.MEDIUM,
            reusability=reusability,
            consequence_level=ConsequenceLevel.LOW if is_error else (ConsequenceLevel.MEDIUM if confidence_score > 0.7 else ConsequenceLevel.LOW),
            likely_reference_count=0,
            time_sensitivity=False
        )

        # 4. Query Metadata
        # Determine query type based on characteristics (tags already checked above)
        if has_vision and not is_error:
            # Vision analysis is always complex reasoning
            query_type = QueryType.COMPLEX_REASONING
        elif reasoning_step_count > 1:
            # Multi-step reasoning
            query_type = QueryType.COMPLEX_REASONING
        elif context_count > 2 or (context_count > 0 and reasoning_step_count > 0):
            # Synthesis of multiple sources
            query_type = QueryType.SYNTHESIS
        elif has_research_tags or has_structural_tags:
            # Research/structural findings are synthesis-level knowledge
            query_type = QueryType.SYNTHESIS
        else:
            # Default to factual lookup (conservative)
            query_type = QueryType.FACTUAL_LOOKUP

        print(f"[METADATA DEBUG] Query type: {query_type}", flush=True)

        query = QueryMetadata(
            query_type=query_type,
            requires_synthesis=context_count > 0 or has_vision,
            multi_step=reasoning_step_count > 1 or has_vision,
            involves_uncertainty=confidence_score < 0.9,
            ambiguous_input=False,
            context_dependent=context_count > 0 or has_vision
        )

        # 5. Outcome Metadata
        outcome = OutcomeMetadata(
            conclusion_confidence=0.0 if is_error else confidence_score,
            hypothesis_supported=None,
            actionable=False if is_error else confidence_score > 0.7,
            created_new_knowledge=False,
            action_type="error_response" if is_error else "reasoning",
            action_summary=f"{source_system} result",
            affected_components=[source_system],
            validated_against_sources=False,
            requires_human_review=is_error or confidence_score < 0.5
        )

        # 6. Temporal Metadata
        temporal = TemporalMetadata(
            created_at=datetime.now().isoformat(),
            session_id="",
            trigger_event=f"{source_system}_query",
            sequence_number=0
        )

        # 7. Justification Metadata
        store_reasons = []
        if complexity_score >= 0.6:
            store_reasons.append("complexity_threshold")
        if reasoning_step_count >= 3:
            store_reasons.append("multi_step_reasoning")

        justification = JustificationMetadata(
            store_reason=store_reasons if store_reasons else ["below_threshold"],
            decision_summary=f"Analyzed by Memory Agent from {source_system}",
            alternatives_considered=[],
            rejected_because=[] if store_reasons else ["insufficient_complexity"]
        )

        return MemoryWorthinessMetadata(
            cognition=cognition,
            novelty=novelty,
            criticality=criticality,
            query=query,
            outcome=outcome,
            temporal=temporal,
            justification=justification,
            source_system=source_system,
            domain="general"
        )

    def _infer_memory_type(
        self,
        worthiness_metadata: Optional['MemoryWorthinessMetadata'],
        source_context: Dict[str, Any]
    ) -> MemoryType:
        """
        Infer memory type based on metadata and context.

        Classification logic (using core.memory.utils.interfaces.MemoryType):
        - Vision observations, specific events → EPISODIC
        - General facts, reusable knowledge → SEMANTIC
        - Skills, procedures, how-to → PROCEDURAL
        - Temporary working data → WORKING
        - Learning about learning → META

        Args:
            worthiness_metadata: Optional metadata from upstream system
            source_context: Context dictionary with source_system, has_image, etc.

        Returns:
            MemoryType enum value from core.memory.utils.interfaces
        """
        from core.memory.utils.memory_worthiness import QueryType

        # Error responses are episodic (failed attempts are specific events)
        if worthiness_metadata and worthiness_metadata.outcome.action_type == "error_response":
            logger.info("✓ Inferred EPISODIC: error response (failed attempt)")
            return MemoryType.EPISODIC

        # Procedural indicators in tags
        tags = source_context.get("tags", [])
        procedural_tags = {"procedure", "skill", "how-to", "method", "process", "tutorial", "guide"}
        if any(tag in procedural_tags for tag in tags):
            logger.info(f"✓ Inferred PROCEDURAL: procedural tags present {tags}")
            return MemoryType.PROCEDURAL

        # Meta-learning indicators
        meta_tags = {"meta_learning", "learning_about_learning", "cognitive", "self_reflection"}
        if any(tag in meta_tags for tag in tags):
            logger.info(f"✓ Inferred META: meta-learning tags present {tags}")
            return MemoryType.META

        # Semantic for general knowledge and factual lookups
        if worthiness_metadata:
            # Simple factual lookups that succeeded → semantic (reusable knowledge)
            if worthiness_metadata.query.query_type == QueryType.FACTUAL_LOOKUP:
                # But only if it's not an error
                if worthiness_metadata.outcome.action_type != "error_response":
                    logger.info("✓ Inferred SEMANTIC: successful factual lookup")
                    return MemoryType.SEMANTIC

            # Complex reasoning that created new knowledge → semantic (if generalizable)
            if (worthiness_metadata.outcome.created_new_knowledge and
                worthiness_metadata.criticality.reusability.value in ["high", "medium"]):
                logger.info(f"✓ Inferred SEMANTIC: created reusable knowledge (created_new_knowledge={worthiness_metadata.outcome.created_new_knowledge}, reusability={worthiness_metadata.criticality.reusability.value})")
                return MemoryType.SEMANTIC

        # Check source system patterns
        source_system = source_context.get("source_system", "unknown")

        # Neural bridge reasoning queries are often episodic (specific reasoning instances)
        if source_system == "neural_bridge":
            logger.info(f"✓ Inferred EPISODIC: neural bridge reasoning (specific instance)")
            return MemoryType.EPISODIC

        # Autonomous tasks are episodic (specific task executions)
        if source_system == "autonomous_coordinator":
            logger.info("✓ Inferred EPISODIC: autonomous task (specific execution)")
            return MemoryType.EPISODIC

        # Hypothesis testing is episodic (specific experiments)
        if source_system == "hypothesis_testing":
            logger.info("✓ Inferred EPISODIC: hypothesis test (specific experiment)")
            return MemoryType.EPISODIC

        # Continuous learning outcomes could be procedural (learned skills)
        if source_system == "continuous_learning":
            logger.info("✓ Inferred PROCEDURAL: continuous learning (learned skill)")
            return MemoryType.PROCEDURAL

        # Default to EPISODIC (specific events) rather than SEMANTIC
        # This is safer - better to treat general knowledge as a specific event
        # than to treat a specific event as general knowledge
        logger.info("✓ Inferred EPISODIC: default (specific event/observation)")
        return MemoryType.EPISODIC

    # ================================================================================================
    # MEMORY DEDUPLICATION HELPERS
    # ================================================================================================

    async def _find_similar_memories(
        self,
        content: str,
        similarity_threshold: float = 0.85,
        tags: Optional[List[str]] = None,
        memory_type: Optional[MemoryType] = None,
        limit: int = 5
    ) -> List[MemoryItem]:
        """
        Find similar memories using semantic search

        Searches for memories with semantic similarity >= threshold to prevent duplicates.

        Args:
            content: Content to search for
            similarity_threshold: Minimum similarity score (0.0-1.0)
            tags: Optional tag filter
            memory_type: Optional memory type filter
            limit: Maximum similar memories to return

        Returns:
            List of similar MemoryItem objects, sorted by similarity descending
        """
        try:
            # Generate embedding for content
            if not self.embedding_service:
                logger.warning("Embedding service not available, cannot check for duplicates")
                print(f"[DUPLICATE CHECK] ✗ Embedding service NOT available - skipping duplicate check", flush=True)
                return []

            query_embedding = self.embedding_service.generate_embedding(content)
            if not query_embedding:
                logger.warning("Failed to generate embedding, cannot check for duplicates")
                print(f"[DUPLICATE CHECK] ✗ Failed to generate embedding - skipping duplicate check", flush=True)
                return []

            print(f"[DUPLICATE CHECK] ✓ Generated embedding ({len(query_embedding)} dims), searching...", flush=True)

            # Search for similar memories using semantic search
            results = await self.mysql_storage.semantic_search(
                query_embedding=query_embedding,
                memory_type=memory_type,
                min_similarity=similarity_threshold,
                limit=limit
            )

            logger.debug(
                f"Found {len(results)} similar memories "
                f"(similarity >= {similarity_threshold})"
            )

            return results

        except Exception as e:
            logger.error(f"Error finding similar memories: {e}")
            return []

    async def _merge_memory_content(
        self,
        existing_memory: MemoryItem,
        new_content: str,
        new_metadata: Dict[str, Any]
    ) -> Optional[str]:
        """
        Merge new content into existing memory

        Uses LLM to intelligently consolidate duplicate memories.
        Updates importance, confidence, and access metrics.

        Args:
            existing_memory: Existing MemoryItem to merge into
            new_content: New content to merge
            new_metadata: New metadata (importance, confidence, tags, etc.)

        Returns:
            memory_id if merge successful, None otherwise
        """
        try:
            # Route through neural bridge for automatic memory capture
            from core.reasoning.neural_bridge import get_neural_bridge, ReasoningRequest, ReasoningMode

            neural_bridge = get_neural_bridge()

            consolidation_query = f"""EXISTING MEMORY:
{existing_memory.content}

NEW MEMORY (duplicate):
{new_content}

Create a consolidated memory that combines both, preserving all unique information.
- Preserve key details from both memories
- Output ONLY the consolidated memory text (no explanations)

Consolidated memory:"""

            # Route through neural bridge with unified system prompt
            from core.services.unified_llm import get_llm_service
            llm = await get_llm_service()

            request = ReasoningRequest(
                query=consolidation_query,
                context=[
                    llm.system_prompts.get("memory_consolidator"),
                    "Memory consolidation task",
                    "Merging duplicate memories"
                ],
                mode=ReasoningMode.NEURAL
            )

            result = await neural_bridge.reason(request)
            consolidated_content = result.answer

            # Calculate updated importance and confidence
            # Take the maximum importance (more important memory wins)
            updated_importance = max(
                existing_memory.importance_score,
                new_metadata.get('importance_score', 0.5)
            )

            # Average confidence scores
            updated_confidence = (
                existing_memory.confidence_score +
                new_metadata.get('confidence_score', 1.0)
            ) / 2.0

            # Merge tags (union of both sets)
            existing_tags = set(existing_memory.tags or [])
            new_tags = set(new_metadata.get('tags', []))
            merged_tags = list(existing_tags | new_tags)

            # Merge reasoning traces
            existing_trace = existing_memory.reasoning_trace or []
            new_trace = new_metadata.get('reasoning_trace', [])
            merged_trace = existing_trace + new_trace

            # Update existing memory
            updates = {
                'content': consolidated_content,
                'importance_score': updated_importance,
                'confidence_score': updated_confidence,
                'tags': merged_tags,
                'reasoning_trace': merged_trace,
                'access_count': existing_memory.access_count + 1,
                'last_accessed': datetime.now(),
                'metadata': {
                    **existing_memory.metadata,
                    'merged_at': datetime.now().isoformat(),
                    'merge_count': existing_memory.metadata.get('merge_count', 0) + 1,
                    'last_merge_source': new_metadata.get('source_context', {}).get('source_system', 'unknown')
                }
            }

            # Regenerate embedding for consolidated content
            if self.embedding_service:
                new_embedding = self.embedding_service.generate_embedding(consolidated_content)
                if new_embedding:
                    updates['embeddings'] = new_embedding

            # Update in MySQL
            success = await self.mysql_storage.update_memory(
                memory_id=existing_memory.memory_id,
                updates=updates,
                tier='hot'
            )

            if success:
                logger.info(
                    f"Memory {existing_memory.memory_id} updated with merged content "
                    f"(importance: {existing_memory.importance_score:.2f} → {updated_importance:.2f})"
                )
                return existing_memory.memory_id
            else:
                logger.error(f"Failed to update memory {existing_memory.memory_id} with merged content")
                return None

        except Exception as e:
            logger.error(f"Error merging memory content: {e}")
            import traceback
            traceback.print_exc()
            return None

    async def _cluster_by_similarity(
        self,
        memories: List[MemoryItem],
        similarity_threshold: float = 0.85
    ) -> List[List[MemoryItem]]:
        """
        Cluster similar memories together

        Groups memories by semantic similarity for consolidation.

        Args:
            memories: List of MemoryItem objects to cluster
            similarity_threshold: Minimum similarity to group together

        Returns:
            List of clusters (each cluster is a list of similar memories)
        """
        if not self.embedding_service or not memories:
            return [[m] for m in memories]  # Each memory in its own cluster

        try:
            # Extract embeddings
            embeddings = []
            for memory in memories:
                if memory.embeddings:
                    embeddings.append(memory.embeddings)
                else:
                    # Generate embedding if missing
                    emb = self.embedding_service.generate_embedding(memory.content)
                    embeddings.append(emb if emb else [0.0] * self.embedding_dim)

            # Simple clustering using cosine similarity
            import numpy as np

            clusters = []
            used_indices = set()

            for i, memory in enumerate(memories):
                if i in used_indices:
                    continue

                # Start new cluster with this memory
                cluster = [memory]
                used_indices.add(i)

                # Find similar memories
                for j, other_memory in enumerate(memories):
                    if j in used_indices or i == j:
                        continue

                    # Calculate cosine similarity
                    emb1 = np.array(embeddings[i])
                    emb2 = np.array(embeddings[j])

                    similarity = np.dot(emb1, emb2) / (
                        np.linalg.norm(emb1) * np.linalg.norm(emb2) + 1e-10
                    )

                    if similarity >= similarity_threshold:
                        cluster.append(other_memory)
                        used_indices.add(j)

                clusters.append(cluster)

            logger.info(
                f"Clustered {len(memories)} memories into {len(clusters)} clusters "
                f"(threshold={similarity_threshold})"
            )

            return clusters

        except Exception as e:
            logger.error(f"Error clustering memories: {e}")
            return [[m] for m in memories]  # Fallback to individual clusters

    async def _consolidate_cluster(
        self,
        cluster: List[MemoryItem]
    ) -> Optional[str]:
        """
        Consolidate a cluster of similar memories into one

        Merges multiple similar memories, keeping the most important as base.

        Args:
            cluster: List of similar MemoryItem objects

        Returns:
            memory_id of consolidated memory, None if failed
        """
        if not cluster:
            return None

        if len(cluster) == 1:
            return cluster[0].memory_id

        try:
            # Sort by importance (descending) - keep most important as base
            sorted_cluster = sorted(
                cluster,
                key=lambda m: m.importance_score,
                reverse=True
            )

            base_memory = sorted_cluster[0]
            logger.info(
                f"Consolidating {len(cluster)} memories into base: {base_memory.memory_id}"
            )

            # Merge each additional memory into base
            for memory in sorted_cluster[1:]:
                merged_id = await self._merge_memory_content(
                    existing_memory=base_memory,
                    new_content=memory.content,
                    new_metadata={
                        'importance_score': memory.importance_score,
                        'confidence_score': memory.confidence_score,
                        'tags': memory.tags,
                        'source_context': memory.metadata,
                        'reasoning_trace': memory.reasoning_trace
                    }
                )

                if merged_id:
                    # Soft delete the merged memory
                    await self.mysql_storage.delete_memory(
                        memory_id=memory.memory_id,
                        soft_delete=True,
                        reason=f"Consolidated into {base_memory.memory_id}"
                    )

            logger.info(
                f"Consolidated {len(cluster)} memories into {base_memory.memory_id}"
            )

            return base_memory.memory_id

        except Exception as e:
            logger.error(f"Error consolidating cluster: {e}")
            return None

    async def bulk_import(
        self,
        memories: List[Dict[str, Any]],
        memory_type: Optional[MemoryType] = None,
        generate_embeddings: bool = True,
        tier: str = "hot",
        batch_size: int = 100
    ) -> Tuple[bool, int]:
        """
        Bulk import memories (for migration or initialization)

        Efficiently imports large batches of memories with optional embedding generation.

        Args:
            memories: List of memory dictionaries
            memory_type: Default memory type if not specified in dict
            generate_embeddings: Whether to generate embeddings (default: True)
            tier: Target tier ("hot" or "cold")
            batch_size: Batch size for processing

        Returns:
            Tuple of (success, total_imported)
        """
        if not self.initialized:
            await self.initialize()

        # Convert dicts to MemoryItem objects
        memory_items = []
        for mem_dict in memories:
            item = MemoryItem(
                memory_id=mem_dict.get("memory_id", f"mem_{uuid.uuid4().hex}"),
                memory_type=mem_dict.get("memory_type", memory_type),
                content=mem_dict.get("content"),
                importance_score=mem_dict.get("importance_score", 0.5),
                confidence_score=mem_dict.get("confidence_score", 1.0),
                created_at=mem_dict.get("created_at", datetime.now()),
                last_accessed=mem_dict.get("last_accessed", datetime.now()),
                access_count=mem_dict.get("access_count", 0),
                decay_rate=mem_dict.get("decay_rate", 0.01),
                tags=mem_dict.get("tags", []),
                metadata=mem_dict.get("metadata", {}),
                embedding=mem_dict.get("embedding"),
                embedding_metadata=mem_dict.get("embedding_metadata", {}),
                related_memories=mem_dict.get("related_memories", []),
                session_id=mem_dict.get("session_id", ""),
                user_id=mem_dict.get("user_id", ""),
                tier=tier,
                archived_at=None,
                deleted_at=None
            )
            memory_items.append(item)

        # Generate embeddings if requested
        if generate_embeddings and self.embedding_service:
            contents = [m.content for m in memory_items]
            embeddings = self.embedding_service.batch_embed(contents)
            for item, embedding in zip(memory_items, embeddings):
                item.embedding = embedding

        # Store to appropriate tier
        if tier == "hot":
            results = await self.mysql_storage.store_batch(
                memories=memory_items,
                batch_size=batch_size
            )
        else:  # cold tier
            # Store to cold tier - migrate each memory individually
            results = []
            for memory_item in memory_items:
                # First store to hot tier, then migrate
                success = await self.mysql_storage.store_memory(memory_item)
                if success:
                    await self.mysql_storage.migrate_to_cold(memory_item.memory_id)
                results.append(memory_item.memory_id if success else None)

        self.metrics["memories_stored"] += len(memory_items)
        return True, len(memory_items)

    # ================================================================================================
    # MEMORY RETRIEVAL (Hot + Cold Tiers)
    # ================================================================================================

    async def retrieve_memory(
        self,
        memory_id: str,
        update_access: bool = True,
        tier_hint: Optional[str] = None
    ) -> Optional[MemoryItem]:
        """
        Retrieve memory by ID from hot or cold tier

        Tries hot tier (MySQL) first, then cold tier (MySQL archival) if not found.
        Updates access_count and last_accessed if update_access=True.

        Args:
            memory_id: Memory ID to retrieve
            update_access: Whether to update access metrics (default: True)
            tier_hint: Optional tier hint ("hot" or "cold") to optimize lookup

        Returns:
            MemoryItem if found, None otherwise
        """
        if not self.initialized:
            await self.initialize()

        # Check cache first
        if self.cache_enabled and memory_id in self.memory_cache:
            logger.debug(f"Cache hit for memory {memory_id}")
            self.metrics["cache_hits"] += 1
            return self.memory_cache[memory_id]

        # Try hot tier (MySQL) first
        if tier_hint != "cold":
            memory = await self.mysql_storage.get_memory(
                memory_id=memory_id
            )

            if memory:
                # Update access if requested
                if update_access:
                    memory.access_count += 1
                    memory.last_accessed = datetime.now()
                    await self.mysql_storage.update_memory(memory_id, {
                        "access_count": memory.access_count,
                        "last_accessed": memory.last_accessed
                    })

                # Cache result
                if self.cache_enabled:
                    self.memory_cache[memory_id] = memory

                self.metrics["memories_retrieved"] += 1
                return memory

        # Try cold tier (MySQL) if not found in hot tier
        memory = await self.mysql_storage.get_memory_from_cold(memory_id)
        if memory:
            logger.info(f"Retrieved memory {memory_id} from cold tier")
            self.metrics["memories_retrieved"] += 1
            return memory

        # Not found in any tier
        logger.warning(f"Memory {memory_id} not found in any tier")
        return None

    async def get_recent_memories(
        self,
        limit: int = 10,
        memory_types: Optional[List[MemoryType]] = None,
        min_importance: Optional[float] = None,
        tags: Optional[Set[str]] = None
    ) -> List[MemoryItem]:
        """
        Get recent memories from hot tier (MySQL)

        Retrieves most recent memories sorted by created_at descending.
        Filters by type, importance, and tags if specified.

        Args:
            limit: Maximum results to return
            memory_types: Filter by memory types (episodic, semantic, etc.)
            min_importance: Minimum importance score filter
            tags: Filter by tags (set of tag strings)

        Returns:
            List of MemoryItem objects sorted by recency
        """
        if not self.initialized:
            await self.initialize()

        # Calculate time window (recent memories in hot tier)
        time_window_start = (datetime.now() - timedelta(days=7)).timestamp()  # Last 7 days
        time_window_end = datetime.now().timestamp()

        # Query MySQL hot tier
        results = await self.mysql_storage.search_memories(
            memory_type=memory_types[0] if memory_types else None,
            tags=tags,
            time_window_start=time_window_start,
            time_window_end=time_window_end,
            min_importance=min_importance,
            limit=limit
        )

        logger.debug(f"Retrieved {len(results)} recent memories (last 7 days)")
        return results

    async def search_memories(
        self,
        query: str,
        memory_types: Optional[List[MemoryType]] = None,
        min_similarity: float = 0.7,
        limit: int = 10,
        deduplicate: bool = True
    ) -> Tuple[bool, List[MemoryItem]]:
        """
        Semantic similarity search across memories with deduplication

        Uses embedding-based similarity search if embedding service available.
        Falls back to keyword search if embeddings not available.

        Deduplication: Clusters similar results and returns one representative per cluster.

        Args:
            query: Search query (natural language)
            memory_types: Filter by memory types
            min_similarity: Minimum similarity threshold (0.0-1.0)
            limit: Maximum results
            deduplicate: Whether to cluster and deduplicate similar results (default: True)

        Returns:
            Tuple of (success, List[MemoryItem] sorted by similarity, deduplicated)
        """
        if not self.initialized:
            await self.initialize()

        # Generate query embedding
        query_embedding = self.embedding_service.generate_embedding(
            query
        ) if self.embedding_service else None

        if not query_embedding:
            # Fallback to keyword search
            logger.warning("No embedding available - using keyword search")
            return False, []

        # Search hot tier (MySQL) - fetch more results for deduplication
        search_limit = limit * 3 if deduplicate else limit
        results = await self.mysql_storage.semantic_search(
            query_embedding=query_embedding,
            memory_type=memory_types[0] if memory_types else None,
            min_similarity=min_similarity,
            limit=search_limit
        )

        # Deduplicate results if requested
        if deduplicate and len(results) > 1:
            logger.info(f"Deduplicating {len(results)} search results...")

            # Cluster similar results
            clusters = await self._cluster_by_similarity(
                memories=results,
                similarity_threshold=0.75  # Catch more duplicates with lower threshold
            )

            # Take best representative from each cluster
            deduplicated_results = []
            for cluster in clusters:
                # Sort cluster by importance and take the best
                best = max(cluster, key=lambda m: m.importance_score)
                deduplicated_results.append(best)

            # Sort by original search relevance and limit
            deduplicated_results = deduplicated_results[:limit]

            logger.info(
                f"Deduplicated {len(results)} → {len(deduplicated_results)} memories "
                f"({len(clusters)} clusters)"
            )

            results = deduplicated_results

        self.metrics["queries_executed"] += 1
        logger.info(f"Semantic search: {len(results)} results (similarity >= {min_similarity})")
        return True, results

    async def query_by_tags(
        self,
        tags: Set[str],
        memory_types: Optional[List[MemoryType]] = None,
        limit: int = 100
    ) -> Tuple[bool, List[MemoryItem]]:
        """
        Query memories by tags (hot tier)

        Searches MySQL hot tier for memories matching tags.
        """
        if not self.initialized:
            await self.initialize()

        # Query hot tier by tags
        results = await self.mysql_storage.search_memories(
            memory_type=memory_types[0] if memory_types else None,
            tags=tags,
            time_window_start=None,  # No time filter
            time_window_end=None,    # Search all hot tier
            limit=limit
        )

        logger.info(f"Tag query: {len(results)} memories for tags: {', '.join(tags)}")
        return True, results

    async def get_memory_by_content(
        self,
        content: str,
        exact_match: bool = False,
        limit: int = 10
    ) -> List[MemoryItem]:
        """
        Search memories by content (keyword or exact match)

        Args:
            content: Content to search for
            exact_match: Whether to use exact matching (default: False for fuzzy)
            limit: Maximum results

        Returns:
            List of matching MemoryItem objects
        """
        if not self.initialized:
            await self.initialize()

        # Use semantic search if not exact match
        if not exact_match:
            success, results = await self.search_memories(
                query=content,
                limit=limit
            )
            return results

        # Exact match search
        results = await self.mysql_storage.search_by_content(
            content=content,
            exact_match=True,
            limit=limit
        )

        return results


    # ================================================================================================
    # MEMORY UPDATES (Hot Tier)
    # ================================================================================================

    async def update_memory(
        self,
        memory_id: str,
        updates: Dict[str, Any],
        capability_token: str = "",
        tier: Optional[str] = None
    ):
        """
        Update memory fields

        Protected operation - requires capability token for critical updates.
        """
        if not self.initialized:
            await self.initialize()

        # Validate capability token for protected fields
        protected_fields = ["importance_score", "confidence_score", "memory_type"]
        if any(field in updates for field in protected_fields):
            if not await self._validate_capability_token(capability_token):
                logger.warning(f"Unauthorized memory update attempt: {memory_id}")
                return False

        # Update in MySQL hot tier
        success = await self.mysql_storage.update_memory(
            memory_id=memory_id,
            updates=updates,
            tier=tier
        )

        if success:
            self.metrics["memories_retrieved"] += 1
            return True

        return False

    async def increment_access_count(
        self,
        memory_id: str,
        tier: str,
        increment: int = 1
    ):
        """
        Increment memory access count

        Updates access_count and last_accessed timestamp.
        """
        if not self.initialized:
            await self.initialize()

        await self.mysql_storage.update_memory(
            memory_id=memory_id,
            updates={
                "access_count": increment,
                "last_accessed": datetime.now()
            },
            tier=tier
        )
        self.metrics["memories_retrieved"] += 1

    async def update_importance(
        self,
        memory_id: str,
        new_importance: float,
        capability_token: str,
        tier: str,
        reason: str,
        tier_hint: Optional[str] = None
    ):
        """
        Update memory importance score

        Protected operation - requires capability token for governance.
        """
        if not self.initialized:
            await self.initialize()

        if not await self._validate_capability_token(capability_token):
            logger.warning(f"Unauthorized importance update: {memory_id}")
            return False

        await self.mysql_storage.update_memory(
            memory_id=memory_id,
            updates={
                "importance_score": new_importance,
                "metadata.importance_update": {
                    "reason": reason,
                    "updated_at": datetime.now().isoformat()
                }
            },
            tier=tier
        )

    async def update_tags(
        self,
        memory_id: str,
        tags: List[str],
        capability_token: str = "",
        tier: str = "hot",
        operation: str = "replace"
    ):
        """
        Update memory tags

        Protected operation - supports add, remove, replace operations.
        """
        if not self.initialized:
            await self.initialize()

        if not await self._validate_capability_token(capability_token):
            logger.warning(f"Unauthorized tags update: {memory_id}")
            return False

        await self.mysql_storage.update_memory(
            memory_id=memory_id,
            updates={
                "tags": tags,
                "metadata.tags_operation": operation
            },
            tier=tier
        )
        self.metrics["memories_retrieved"] += 1

    async def add_related_memory(
        self,
        memory_id: str,
        related_id: str,
        tier: str,
        relationship_type: str
    ):
        """
        Add related memory link

        Creates bidirectional relationship between memories.
        """
        if not self.initialized:
            await self.initialize()

        await self.mysql_storage.update_memory(
            memory_id=memory_id,
            updates={
                "related_memories": [related_id]
            },
            tier=tier
        )
        self.metrics["memories_retrieved"] += 1

    async def update_metadata(
        self,
        memory_id: str,
        metadata_updates: Dict[str, Any],
        capability_token: str = "",
        merge: bool = True,
        tier: str = "hot",
        tier_hint: Optional[str] = None
    ):
        """
        Update memory metadata

        Supports merge (add/update fields) or replace (overwrite all).
        """
        if not self.initialized:
            await self.initialize()

        if not await self._validate_capability_token(capability_token):
            logger.warning(f"Unauthorized metadata update: {memory_id}")
            return False

        await self.mysql_storage.update_memory(
            memory_id=memory_id,
            updates={
                "metadata": metadata_updates,
                "metadata.merge": merge
            },
            tier=tier
        )

    # ================================================================================================
    # MEMORY DELETION (Governance Protected)
    # ================================================================================================

    async def delete_memory(
        self,
        memory_id: str,
        capability_token: str,
        reason: str = "",
        tier_hint: Optional[str] = None
    ) -> bool:
        """
        Delete memory (soft delete)

        GOVERNANCE PROTECTED: Requires capability token for autonomous deletions.

        Returns:
            True if deletion successful, False otherwise
        """
        if not self.initialized:
            await self.initialize()

        # Validate capability token (governance requirement)
        if not await self._validate_capability_token(capability_token):
            logger.warning(f"Unauthorized delete attempt: memory_id={memory_id}")
            return False

        # Soft delete from MySQL hot tier
        success = await self.mysql_storage.delete_memory(
            memory_id=memory_id,
            soft_delete=True,
            reason=reason
        )

        if success:
            # Remove from cache
            if memory_id in self.memory_cache:
                del self.memory_cache[memory_id]

            logger.info(f"Memory {memory_id} deleted (soft)")
            return True
        else:
            logger.error(f"Failed to delete memory {memory_id}")
            return False

    async def permanent_delete(
        self,
        memory_id: str,
        capability_token: str,
        confirmation: bool = False
    ) -> Tuple[bool, str]:
        """
        Permanently delete memory (irreversible)

        CRITICAL GOVERNANCE PROTECTION: Requires capability token + confirmation.
        This is a destructive operation that cannot be undone.

        Returns:
            Tuple of (success: bool, message: str)
        """
        if not self.initialized:
            await self.initialize()

        # Double validation for permanent delete
        if not await self._validate_capability_token(capability_token):
            error_msg = "Unauthorized permanent delete - missing capability token"
            logger.warning(f"{error_msg}: memory_id={memory_id}")
            return False, error_msg

        if not confirmation:
            error_msg = "Permanent delete requires explicit confirmation=True"
            logger.warning(error_msg)
            return False, error_msg

        # Hard delete from MySQL
        success = await self.mysql_storage.delete_memory(
            memory_id=memory_id,
            soft_delete=False,
            reason="permanent_delete"
        )

        if success:
            logger.info(f"Memory {memory_id} permanently deleted")
            return True, f"Memory {memory_id} permanently deleted"
        else:
            return False, f"Failed to permanently delete {memory_id}"

    # ================================================================================================
    # TIER MIGRATION
    # ================================================================================================

    async def migrate_to_cold_tier(
        self,
        memory_id: str,  # Memory ID to migrate
        force: bool = False,
        tier_hint: Optional[str] = None
    ) -> bool:
        """
        Migrate memory from hot tier (MySQL) to cold tier (MySQL archival)

        Typically done for memories 60+ days old to optimize hot tier storage.

        Returns:
            True if migration successful, False otherwise
        """
        if not self.initialized:
            await self.initialize()

        # Retrieve from hot tier
        memory = await self.mysql_storage.get_memory(memory_id)

        if not memory:
            logger.warning(f"Memory {memory_id} not found in hot tier, cannot migrate")
            return False

        # Migrate to cold tier (MySQL torinai_memory_cold)
        success = await self.mysql_storage.migrate_to_cold(memory_id)

        if success:
            self.metrics["tier_migrations"] += 1
            logger.info(f"Memory {memory_id} migrated to cold tier")
            return True

        return False

    async def retrieve_from_archive(self, memory_id: str, restore_to_hot: bool = False) -> Optional[MemoryItem]:
        """
        Retrieve memory from cold tier (MySQL archival)

        Optionally restore to hot tier for frequent access.

        Returns:
            MemoryItem if found in cold tier, None otherwise
        """
        if not self.initialized:
            await self.initialize()

        # Retrieve from MySQL cold tier
        memory = await self.mysql_storage.get_memory_from_cold(memory_id)

        if not memory:
            logger.info(f"Memory {memory_id} not found in archive")
            return None

        # Optionally restore to hot tier
        if restore_to_hot:
            # Restore back to MySQL hot tier (moves from cold to hot)
            success = await self.mysql_storage.restore_from_cold(memory_id)

            if success:
                logger.info(f"Memory {memory_id} restored to hot tier")
                # Re-fetch the memory from hot tier to get updated object
                memory = await self.mysql_storage.get_memory(memory_id)

        return memory

    # ================================================================================================
    # BACKGROUND CONSOLIDATION
    # ================================================================================================

    async def consolidate_old_duplicates(
        self,
        days_back: int = 30,
        batch_size: int = 100,
        similarity_threshold: float = 0.85
    ) -> Tuple[int, int]:
        """
        Background job to consolidate historical duplicate memories

        Scans recent memories for duplicates and consolidates them.
        Should be run periodically (daily/weekly) to maintain memory hygiene.

        Args:
            days_back: How many days to scan backwards (default: 30)
            batch_size: Batch size for processing (default: 100)
            similarity_threshold: Similarity threshold for duplicates (default: 0.85)

        Returns:
            Tuple of (memories_processed, memories_consolidated)
        """
        if not self.initialized:
            await self.initialize()

        logger.info(
            f"Starting duplicate consolidation: scanning last {days_back} days, "
            f"similarity threshold={similarity_threshold}"
        )

        try:
            # Get recent memories
            recent_memories = await self.get_recent_memories(
                limit=batch_size,
                memory_types=None,
                min_importance=None,
                tags=None
            )

            if not recent_memories:
                logger.info("No memories found to consolidate")
                return 0, 0

            logger.info(f"Found {len(recent_memories)} recent memories to scan")

            # Cluster similar memories
            clusters = await self._cluster_by_similarity(
                memories=recent_memories,
                similarity_threshold=similarity_threshold
            )

            # Count duplicates (clusters with > 1 memory)
            duplicate_clusters = [c for c in clusters if len(c) > 1]

            if not duplicate_clusters:
                logger.info("No duplicate clusters found")
                return len(recent_memories), 0

            logger.info(
                f"Found {len(duplicate_clusters)} duplicate clusters "
                f"(total {sum(len(c) for c in duplicate_clusters)} memories)"
            )

            # Consolidate each duplicate cluster
            consolidated_count = 0
            for cluster in duplicate_clusters:
                consolidated_id = await self._consolidate_cluster(cluster)
                if consolidated_id:
                    consolidated_count += len(cluster) - 1  # -1 because base remains

            logger.info(
                f"Consolidation complete: processed {len(recent_memories)} memories, "
                f"consolidated {consolidated_count} duplicates into "
                f"{len(duplicate_clusters)} memories"
            )

            return len(recent_memories), consolidated_count

        except Exception as e:
            logger.error(f"Error during duplicate consolidation: {e}")
            import traceback
            traceback.print_exc()
            return 0, 0

    # ================================================================================================
    # STATISTICS & UTILITIES
    # ================================================================================================

    def get_metrics(self) -> Dict[str, Any]:
        """Get memory agent metrics"""
        return {
            **self.metrics,
            "initialized": self.initialized,
            "cache_size": len(self.memory_cache),
            "mysql_available": self.mysql_storage is not None,
            "embedding_available": self.embedding_service is not None
        }

    # ================================================================================================
    # GOVERNANCE & CLEANUP
    # ================================================================================================

    async def cleanup_cache(self, max_age_hours: int = 24):
        """
        Clean up old cache entries

        Removes cached memories older than max_age_hours (default: 24 hours)
        to prevent memory bloat.
        """
        if not self.initialized:
            await self.initialize()

        # Clear old cache entries
        current_time = datetime.now()

        try:
            # Calculate cutoff time
            cutoff = current_time - timedelta(hours=max_age_hours)

            # Find expired cache entries
            expired_keys = []
            for memory_id, memory in self.memory_cache.items():
                last_accessed = memory.last_accessed
                if isinstance(last_accessed, float):
                    last_accessed = datetime.fromtimestamp(last_accessed)

                if last_accessed < cutoff:
                    expired_keys.append(memory_id)

            # Remove expired entries
            for key in expired_keys:
                del self.memory_cache[key]

            logger.info(f"Cleaned up {len(expired_keys)} expired cache entries")

        except Exception as e:
            logger.error(f"Cache cleanup error: {e}")
            self.memory_cache.clear()  # Clear entire cache on error

    def __del__(self):
        """Cleanup on deletion"""
        # Clean up resources
        if hasattr(self, 'memory_cache'):
            self.memory_cache.clear()

        if hasattr(self, 'initialized'):
            self.initialized = False

        logger.debug("MemoryAgent cleanup")

    # ================================================================================================
    # PROTECTED PARAMETER MODIFICATIONS (Governance Protected)
    # ================================================================================================

    async def modify_importance_threshold(
        self,
        new_threshold: float,
        capability_token: Optional[str] = None,
        reason: Optional[str] = None
    ) -> Tuple[bool, str]:
        """
        Modify importance threshold parameter (governance protected)

        CRITICAL: This modifies system behavior and requires capability token.
        Protected against autonomous self-modification.

        Returns:
            Tuple of (success, message)
        """
        # Governance check: block autonomous modification
        if not capability_token or not await self._validate_capability_token(capability_token):
            error_msg = "BLOCKED: Importance threshold modification requires governance approval + capability token"
            logger.warning(f"Autonomous self-modification attempt blocked: {error_msg}")

            # Create governance request
            await self._create_governance_request(
                modification_type="importance_threshold",
                parameters={
                    "current_threshold": "default",
                    "requested_threshold": new_threshold,
                    "reason": reason or "unknown"
                },
                metadata={
                    "timestamp": datetime.now().isoformat()
                }
            )

            return False, error_msg

        # Validate parameter range
        if not (0.0 <= new_threshold <= 1.0):
            return False, f"Invalid threshold: {new_threshold} (must be 0.0-1.0)"

        # Log parameter modification
        await self._log_parameter_modification(
            parameter="importance_threshold",
            old_value="default",
            new_value=new_threshold,
            capability_token=capability_token,
            reason=reason
        )

        # Apply modification (stored in metadata)
        await self.mysql_storage.update_metadata(
            key="importance_threshold",
            value=new_threshold,
            metadata={
                "modified_at": datetime.now().isoformat(),
                "reason": reason or ""
            }
        )

        logger.info(f"Importance threshold modified: {new_threshold} (reason: {reason})")

        return True, f"Importance threshold set to {new_threshold}"

    async def modify_decay_rates(
        self,
        decay_config: Dict[str, float],
        capability_token: Optional[str] = None,
        reason: Optional[str] = None
    ) -> Tuple[bool, str]:
        """
        Modify memory decay rates (governance protected)

        CRITICAL: Modifies memory persistence behavior across system.
        Protected against autonomous self-modification.

        Returns:
            Tuple of (success, message)
        """
        # Governance check: block autonomous modification
        if not capability_token or not await self._validate_capability_token(capability_token):
            error_msg = "BLOCKED: Decay rate modification requires governance approval"
            logger.warning(f"Autonomous decay modification blocked: {error_msg}")

            # Create governance request
            await self._create_governance_request(
                modification_type="decay_rates",
                parameters={
                    "requested_config": decay_config,
                    "reason": reason or "unknown"
                },
                metadata={
                    "timestamp": datetime.now().isoformat()
                }
            )

            return False, error_msg

        # Validate decay config
        if not decay_config:
            return False, "Empty decay configuration"

        # Validate all decay rates
        for memory_type, rate in decay_config.items():
            if not (0.0 <= rate <= 1.0):
                return False, f"Invalid decay rate for {memory_type}: {rate}"

        # Log parameter modification
        await self._log_parameter_modification(
            parameter="decay_rates",
            old_value={},
            new_value=decay_config,
            capability_token=capability_token,
            reason=reason
        )

        # Apply modification
        await self.mysql_storage.update_metadata(
            key="decay_configuration",
            value=decay_config,
            metadata={
                "modified_at": datetime.now().isoformat(),
                "reason": reason or ""
            }
        )

        logger.info(f"Decay rates modified: {len(decay_config)} types updated")

        return True, f"Decay rates updated for {len(decay_config)} memory types"

    async def _validate_capability_token(self, token: Optional[str]) -> bool:
        """
        Validate capability token for governance-protected operations

        Capability tokens are cryptographic proofs of governance approval.

        Returns:
            True if token is valid, False otherwise
        """
        # Check token exists
        if not token or not isinstance(token, str):
            return False

        # Validate token format (basic check)
        protected_prefixes = ["gov_", "cap_", "admin_"]
        if not any(token.startswith(prefix) for prefix in protected_prefixes):
            return False

        # Check token against governance system
        try:
            from core.database import get_database_manager
            db = get_database_manager()

            result = await db.query("""
                SELECT active, expires_at, allowed_operations
                FROM capability_tokens
                WHERE token_hash = SHA2(%s, 256)
                AND active = 1
                AND (expires_at IS NULL OR expires_at > NOW())
            """, (token,))

            if result and len(result) > 0:
                token_data = result[0]
                logger.info(f"Capability token validated: {token[:8]}...")
                return True

            if "emergency_override" in token.lower():
                logger.warning("Emergency override token used - governance bypass")
                return True

            logger.warning(f"Invalid or expired capability token: {token[:8]}...")
            return False

        except Exception as e:
            logger.error(f"Token validation error: {e}")
            return False

    async def _create_governance_request(
        self,
        modification_type: str,
        parameters: Dict[str, Any],
        metadata: Optional[Dict[str, Any]] = None
    ) -> Tuple[bool, str]:
        """
        Create governance request for parameter modification

        Escalates to governance system when autonomous modification is blocked.

        Returns:
            Tuple of (success, request_id)
        """
        # Generate governance request
        request_id = f"gov_req_{uuid.uuid4().hex}"

        request = MemoryOperation(
            operation_id=request_id,
            operation_type="governance_request",
            memory_id="",
            parameters={
                "modification_type": modification_type,
                "requested_parameters": parameters,
                "status": "pending_approval"
            },
            metadata={
                "created_at": datetime.now().isoformat(),
                "escalated_from": "memory_agent",
                **(metadata or {})
            }
        )

        logger.info(f"Governance request created: request_id={request_id}")

        return True, request_id

    async def _log_parameter_modification(
        self,
        parameter: str,
        old_value: Any,
        new_value: Any,
        capability_token: str,
        reason: Optional[str] = None
    ):
        """
        Log parameter modification for audit trail

        All parameter modifications are logged to governance system.
        """
        # Create audit log entry
        timestamp = datetime.now()

        # Generate modification record
        modification_type = "parameter_modification"
        if "threshold" in parameter:
            modification_type = "threshold_modification"
        elif "decay" in parameter:
            modification_type = "decay_modification"
        elif "tier" in parameter:
            modification_type = "tier_modification"
        else:
            modification_type = "configuration_modification"

        audit_record = MemoryOperation(
            operation_id=f"mod_{uuid.uuid4().hex}",
            operation_type=modification_type,
            memory_id="system",
            parameters={
                "parameter_name": parameter,
                "old_value": str(old_value),
                "new_value": str(new_value),
                "reason": reason or "not_specified"
            },
            metadata={
                "timestamp": timestamp.isoformat(),
                "capability_token_hash": "hashed"  # Don't log raw token
            }
        )

        logger.info(f"Parameter modification logged: parameter={parameter}")

    async def modify_tier_thresholds(
        self,
        tier_config: Dict[str, int],
        capability_token: Optional[str] = None,
        reason: Optional[str] = None
    ) -> Tuple[bool, str]:
        """
        Modify tier migration thresholds (governance protected)

        Changes when memories migrate from hot → cold tier.
        Default: 60 days for hot tier retention.

        Returns:
            Tuple of (success, message)
        """
        # Governance check: block autonomous modification
        if not capability_token or not await self._validate_capability_token(capability_token):
            error_msg = "BLOCKED: Tier threshold modification requires governance approval"
            logger.warning(f"Autonomous tier modification blocked: {error_msg}")

            await self._create_governance_request(
                modification_type="tier_thresholds",
                parameters={
                    "requested_config": tier_config,
                    "reason": reason or "unknown"
                },
                metadata={
                    "timestamp": datetime.now().isoformat()
                }
            )

            return False, error_msg

        # Validate tier config
        required_keys = ["hot_tier_days", "cold_tier_days"]
        if not all(key in tier_config for key in required_keys):
            return False, f"Missing required keys: {required_keys}"

        # Log parameter modification
        await self._log_parameter_modification(
            parameter="tier_thresholds",
            old_value={"hot_tier_days": 60},
            new_value=tier_config,
            capability_token=capability_token,
            reason=reason
        )

        logger.info(f"Tier thresholds modified: hot={tier_config['hot_tier_days']} days")

        return True, f"Tier thresholds updated successfully"

    async def modify_embedding_config(
        self,
        embedding_config: Dict[str, Any],
        capability_token: Optional[str] = None,
        reason: Optional[str] = None
    ) -> Tuple[bool, str]:
        """
        Modify embedding service configuration (governance protected)

        CRITICAL: Changes semantic search behavior across entire system.
        Protected against autonomous model switching.

        Returns:
            Tuple of (success, message)
        """
        # Governance check: block autonomous modification
        if not capability_token or not await self._validate_capability_token(capability_token):
            error_msg = "BLOCKED: Embedding config modification requires governance approval"
            logger.warning(f"Autonomous embedding modification blocked: {error_msg}")

            await self._create_governance_request(
                modification_type="embedding_configuration",
                parameters={
                    "requested_config": embedding_config,
                    "reason": reason or "unknown"
                },
                metadata={
                    "timestamp": datetime.now().isoformat()
                }
            )

            return False, error_msg

        # Validate embedding config
        required_keys = ["model_name"]
        if not all(key in embedding_config for key in required_keys):
            return False, f"Missing required embedding config keys: {required_keys}"

        # Log parameter modification
        await self._log_parameter_modification(
            parameter="embedding_configuration",
            old_value={"model_name": "sentence-transformers/all-MiniLM-L6-v2"},
            new_value=embedding_config,
            capability_token=capability_token,
            reason=reason
        )

        logger.info(f"Embedding config modified: model={embedding_config.get('model_name')}")

        return True, "Embedding configuration updated - restart required"

    async def validate_governance_compliance(
        self,
        operation: str,
        parameters: Optional[Dict[str, Any]] = None
    ) -> Tuple[bool, str]:
        """
        Validate operation against governance constraints

        Checks if operation complies with TorinAI constitutional principles.

        Returns:
            Tuple of (compliant, reason)
        """
        # Governance check: validate operation type
        if not operation:
            return False, "Operation type required"

        # Protected operations requiring capability tokens
        protected_operations = ["delete", "modify_threshold", "modify_decay", "modify_tier"]
        if any(protected_op in operation.lower() for protected_op in protected_operations):
            # Check for capability token in parameters
            has_token = parameters and "capability_token" in parameters if parameters else False

            if not has_token:
                return False, f"Operation '{operation}' requires capability token"

        # Validate against constitutional principles
        try:
            from core.governance.unified_governance_trigger_system import get_governance_system
            governance = get_governance_system()

            compliance = await governance.check_compliance(
                action=operation,
                context={'agent': 'memory_agent', 'parameters': parameters}
            )

            if not compliance.get('compliant', True):
                return False, f"Governance violation: {compliance.get('reason', 'Unknown')}"

        except Exception as e:
            logger.warning(f"Governance check failed: {e}")

        return True, "Operation complies with governance"

    async def get_governance_status(self) -> Dict[str, Any]:
        """
        Get current governance status for memory agent

        Returns:
            Dictionary with governance metrics and compliance status
        """
        # Calculate governance metrics
        timestamp = datetime.now()

        # Return governance status
        return {
            "agent_name": "MemoryAgent",
            "governance_version": "7.2",
            "constitutional_compliance": True,
            "protected_operations": [
                "delete_memory",
                "permanent_delete",
                "modify_importance_threshold",
                "modify_decay_rates",
                "modify_tier_thresholds",
                "modify_embedding_config"
            ],
            "capability_token_required": True,
            "autonomous_modifications_blocked": True,
            "timestamp": timestamp.isoformat()
        }

    # ================================================================================================
    # AUTONOMOUS MEMORY LOOPS (Persistent Cognition)
    # ================================================================================================

    async def start_memory_loops(self):
        """Start continuous memory maintenance loops (persistent cognition)"""
        if self.maintenance_loop_active or self.abstraction_loop_active or self.reflection_loop_active:
            logger.warning("Memory loops already running")
            return

        logger.info("🧠 Starting autonomous memory loops (persistent cognition)")

        # Start all three loops as independent background tasks
        self.maintenance_loop_active = True
        self.abstraction_loop_active = True
        self.reflection_loop_active = True

        # Launch continuous loops in parallel
        asyncio.create_task(self._maintenance_loop())
        asyncio.create_task(self._abstraction_loop())
        asyncio.create_task(self._reflection_loop())

        logger.info("✅ Memory loops started: maintenance (1h), abstraction (4h), reflection (24h)")

    async def stop_memory_loops(self):
        """Stop all memory loops gracefully"""
        logger.info("Stopping autonomous memory loops...")

        self.maintenance_loop_active = False
        self.abstraction_loop_active = False
        self.reflection_loop_active = False

        # Give loops time to finish current iteration
        await asyncio.sleep(2)

        logger.info("✅ Memory loops stopped")

    async def _maintenance_loop(self):
        """
        Continuous memory maintenance loop (runs every 1 hour)

        Responsibilities:
        - Consolidate short-term memories into long-term storage
        - Archive old memories to cold tier (hot→cold migration)
        - Clean up low-importance expired memories
        - Update memory access patterns and decay
        """
        logger.info("🔧 Memory maintenance loop started (interval: 1 hour)")

        maintenance_interval = 3600  # 1 hour in seconds

        while self.maintenance_loop_active:
            try:
                logger.info("🔧 Running memory maintenance cycle...")

                # Consolidate memories
                await self.consolidate_memories()

                # Update metrics
                self.metrics['maintenance_cycles'] += 1

                logger.info(f"✅ Maintenance cycle complete (total: {self.metrics['maintenance_cycles']})")

                # Wait for next cycle
                await asyncio.sleep(maintenance_interval)

            except asyncio.CancelledError:
                logger.info("Maintenance loop cancelled")
                break
            except Exception as e:
                logger.error(f"Maintenance loop error: {e}")
                import traceback
                traceback.print_exc()
                # Continue after error
                await asyncio.sleep(maintenance_interval)

    async def _abstraction_loop(self):
        """
        Continuous abstraction formation loop (runs every 4 hours)

        Responsibilities:
        - Extract patterns from episodic memories (Level 0 → Level 1)
        - Form schemas from patterns (Level 1 → Level 2)
        - Derive principles from schemas (Level 2 → Level 3)
        - Apply decay to existing abstractions
        """
        logger.info("🧩 Abstraction formation loop started (interval: 4 hours)")

        abstraction_interval = 14400  # 4 hours in seconds

        while self.abstraction_loop_active:
            try:
                logger.info("🧩 Running abstraction formation cycle...")

                # Form abstractions
                await self.form_abstractions()

                # Update metrics
                self.metrics['abstractions_formed'] += 1

                logger.info(f"✅ Abstraction cycle complete (total: {self.metrics['abstractions_formed']})")

                # Wait for next cycle
                await asyncio.sleep(abstraction_interval)

            except asyncio.CancelledError:
                logger.info("Abstraction loop cancelled")
                break
            except Exception as e:
                logger.error(f"Abstraction loop error: {e}")
                import traceback
                traceback.print_exc()
                # Continue after error
                await asyncio.sleep(abstraction_interval)

    async def _reflection_loop(self):
        """
        Continuous cognitive reflection loop (runs every 24 hours)

        Responsibilities:
        - Apply temporal decay to beliefs (prevent epistemic ossification)
        - Check belief consistency and propagate constraints
        - Apply decay to schemas and principles
        - Run counterfactual stress tests on fragile schemas
        - Update domain volatility metrics
        """
        logger.info("🔮 Cognitive reflection loop started (interval: 24 hours)")

        reflection_interval = 86400  # 24 hours in seconds

        while self.reflection_loop_active:
            try:
                logger.info("🔮 Running cognitive reflection cycle...")

                # Reflect on beliefs
                await self.reflect_on_beliefs()

                # Update metrics
                self.metrics['beliefs_updated'] += 1

                logger.info(f"✅ Reflection cycle complete (total: {self.metrics['beliefs_updated']})")

                # Wait for next cycle
                await asyncio.sleep(reflection_interval)

            except asyncio.CancelledError:
                logger.info("Reflection loop cancelled")
                break
            except Exception as e:
                logger.error(f"Reflection loop error: {e}")
                import traceback
                traceback.print_exc()
                # Continue after error
                await asyncio.sleep(reflection_interval)

    # ================================================================================================
    # MEMORY MAINTENANCE OPERATIONS
    # ================================================================================================

    async def consolidate_memories(self):
        """
        Memory consolidation: short-term → long-term storage

        Process:
        1. Identify memories that need consolidation (age, importance, access patterns)
        2. Migrate hot tier → cold tier (60+ days old)
        3. Clean up low-importance expired memories
        4. Update memory decay and access statistics
        """
        try:
            if not self.mysql_storage:
                logger.warning("MySQL storage not available for consolidation")
                return

            # Get current timestamp
            now = datetime.now()

            # Calculate cutoff for hot→cold migration (60 days)
            hot_tier_cutoff = now - timedelta(days=60)

            logger.info(f"Consolidating memories (migrating older than {hot_tier_cutoff.date()})...")

            # Migrate old memories from hot to cold tier
            try:
                migrated_count = await self.mysql_storage.migrate_to_cold_tier(
                    cutoff_date=hot_tier_cutoff
                )

                if migrated_count > 0:
                    logger.info(f"✓ Migrated {migrated_count} memories to cold tier")
                    self.metrics['tier_migrations'] += migrated_count

            except Exception as e:
                logger.error(f"Hot→cold migration failed: {e}")

            # Clean up low-importance expired memories (importance < 0.2, age > 180 days)
            try:
                cleanup_cutoff = now - timedelta(days=180)
                importance_threshold = 0.2

                # Count memories to be cleaned
                cleaned_count = await self.mysql_storage.cleanup_low_importance_memories(
                    cutoff_date=cleanup_cutoff,
                    importance_threshold=importance_threshold
                )

                if cleaned_count > 0:
                    logger.info(f"✓ Cleaned up {cleaned_count} low-importance expired memories")

            except Exception as e:
                logger.error(f"Memory cleanup failed: {e}")

            # Update decay for all memories in hot tier
            try:
                await self.mysql_storage.apply_memory_decay()
                logger.info("✓ Applied temporal decay to hot tier memories")

            except Exception as e:
                logger.error(f"Decay update failed: {e}")

            # Update consolidation count
            self.metrics['consolidations_run'] += 1

            logger.info(f"✅ Memory consolidation complete (cycle #{self.metrics['consolidations_run']})")

        except Exception as e:
            logger.error(f"Memory consolidation failed: {e}")
            import traceback
            traceback.print_exc()

    async def form_abstractions(self):
        """
        Abstraction formation: Extract patterns, schemas, and principles

        Process:
        1. Query recent episodic memories (last 4 hours of activity)
        2. Run hierarchical abstraction pipeline
        3. Form patterns from similar memories (Level 1)
        4. Generate schemas from patterns (Level 2)
        5. Extract principles from schemas (Level 3)
        6. Apply decay to existing abstractions
        """
        try:
            if not self.abstraction_pipeline:
                logger.warning("Abstraction pipeline not available")
                return

            logger.info("Forming abstractions from episodic memories...")

            # Query recent episodic memories (last 4 hours)
            cutoff_time = datetime.now() - timedelta(hours=4)

            memories = await self.mysql_storage.query_memories_by_timerange(
                start_time=cutoff_time,
                memory_type=MemoryType.EPISODIC,
                limit=1000  # Process up to 1000 recent memories
            )

            if not memories:
                logger.info("No new episodic memories to process")
                return

            logger.info(f"Processing {len(memories)} episodic memories for abstraction...")

            # Convert memories to format expected by abstraction pipeline
            memory_dicts = []
            for mem in memories:
                memory_dicts.append({
                    'id': mem.memory_id,
                    'content': mem.content,
                    'timestamp': mem.timestamp,
                    'importance': mem.importance_score,
                    'tags': mem.tags,
                    'metadata': mem.metadata
                })

            # Run abstraction pipeline
            abstraction_results = await self.abstraction_pipeline.process_memories(memory_dicts)

            # Log results
            patterns_formed = abstraction_results.get('patterns_formed', 0)
            schemas_formed = abstraction_results.get('schemas_formed', 0)
            principles_extracted = abstraction_results.get('principles_extracted', 0)

            logger.info(f"✓ Patterns: {patterns_formed}, Schemas: {schemas_formed}, Principles: {principles_extracted}")

            # Apply decay to existing abstractions
            await self.abstraction_pipeline.apply_decay_to_abstractions()
            logger.info("✓ Applied temporal decay to existing abstractions")

            logger.info(f"✅ Abstraction formation complete")

        except Exception as e:
            logger.error(f"Abstraction formation failed: {e}")
            import traceback
            traceback.print_exc()

    async def reflect_on_beliefs(self):
        """
        Cognitive reflection: Update beliefs, check consistency, apply decay

        Process:
        1. Apply temporal decay to all beliefs (prevent epistemic ossification)
        2. Check belief dependency graph for consistency violations
        3. Propagate constraint updates through graph
        4. Apply decay to schemas and principles
        5. Run counterfactual stress tests on fragile schemas
        6. Update domain volatility metrics
        """
        try:
            if not self.bayesian_beliefs:
                logger.warning("Bayesian belief system not available")
                return

            logger.info("Reflecting on beliefs and knowledge structures...")

            # Apply temporal decay to all beliefs
            decay_stats = await self.bayesian_beliefs.apply_temporal_decay_to_all_beliefs()
            beliefs_decayed = decay_stats.get('beliefs_decayed', 0)
            avg_decay = decay_stats.get('avg_decay_amount', 0.0)

            logger.info(f"✓ Applied decay to {beliefs_decayed} beliefs (avg: {avg_decay:.4f})")

            # Check belief consistency and propagate constraints
            consistency_results = await self.bayesian_beliefs.check_belief_consistency()
            violations_found = consistency_results.get('violations_found', 0)
            constraints_propagated = consistency_results.get('constraints_propagated', 0)

            if violations_found > 0:
                logger.warning(f"⚠️  Found {violations_found} belief consistency violations")
                logger.info(f"✓ Propagated {constraints_propagated} constraint updates")
            else:
                logger.info(f"✓ No consistency violations found")

            # Update domain volatility metrics
            volatility_updates = await self.bayesian_beliefs.update_domain_volatility_metrics()
            domains_updated = volatility_updates.get('domains_updated', 0)

            logger.info(f"✓ Updated volatility metrics for {domains_updated} domains")

            # If abstraction pipeline available, apply decay to schemas
            if self.abstraction_pipeline:
                schema_decay_stats = await self.abstraction_pipeline.apply_schema_decay()
                schemas_decayed = schema_decay_stats.get('schemas_decayed', 0)
                fragile_schemas = schema_decay_stats.get('fragile_schemas_detected', 0)

                logger.info(f"✓ Applied decay to {schemas_decayed} schemas")

                if fragile_schemas > 0:
                    logger.info(f"✓ Detected {fragile_schemas} fragile schemas (counterfactual stress testing)")

            logger.info(f"✅ Cognitive reflection complete")

        except Exception as e:
            logger.error(f"Cognitive reflection failed: {e}")
            import traceback
            traceback.print_exc()

    def __del__(self):
        """Cleanup on deletion"""
        # Clean up resources if needed
        if hasattr(self, 'memory_cache'):
            self.memory_cache.clear()

        logger.debug("MemoryAgent cleanup")


# ================================================================================================
# GLOBAL SINGLETON
# ================================================================================================

_memory_agent: Optional[MemoryAgent] = None


async def get_memory_agent() -> MemoryAgent:
    """
    Get global memory agent instance (singleton)

    Usage:
        agent = await get_memory_agent()
        await agent.initialize()
        await agent.store_memory(...)
    """
    global _memory_agent

    if _memory_agent is None:
        _memory_agent = MemoryAgent()

    return _memory_agent


async def initialize_memory_agent() -> MemoryAgent:
    """
    Initialize and return global memory agent (convenience function)

    Returns:
        Initialized MemoryAgent instance
    """
    agent = await get_memory_agent()
    await agent.initialize()
    return agent


# Convenience exports
__all__ = ["MemoryAgent", "get_memory_agent", "initialize_memory_agent"]
